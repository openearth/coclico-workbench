{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoCliCo --- Delft Notebook Demonstration (October 2023)\n",
    "\n",
    "Coastal Climate Core Services (CoCliCo) is an European effort to develop an opean web-platform to aid decision making on coastal risk (2021 - 2025). Please have a look at our [website](https://coclicoservices.eu/) to find out more about the project. \n",
    "\n",
    "During this project several datasets will be made available, which can be explored on the platform as well as accessed via cloud-storage buckets. In this notebook, some examples are provided on how to interact with the data using Python, specifically for sea level rise projections from the Intergovernmental Panel on Climate Change (IPCC) at national scales.\n",
    "\n",
    "- Notebook author: Etiënne Kras, 26 July 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software environment\n",
    "\n",
    "- Please install the latest mambaforge package manager by downloading the executable from the [MiniForge GitHub repo](https://github.com/conda-forge/miniforge#mambaforge) for your Operating System. \n",
    "- Clone the [coclicodata GitHub repo](https://github.com/openearth/coclicodata) on your local desktop using Git or GitHub Desktop.\n",
    "- Open a Miniforge Prompt (terminal) on your local desktop. \n",
    "- Navigate to the coclicodata GitHub repo in the Miniforge Prompt and run `mamba env create -f environment.yml`. \n",
    "- This may take a few minutes to complete but once it is finished you will have all required packages to run this notebook installed in your ‘coclico’ environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPCC AR5 & AR6 sea level rise projections \n",
    "\n",
    "Here, we use [IPCC's](https://www.ipcc.ch/) Fifth Assessment Report (AR5, 2013) relative Sea Surface Height (SSH) data processed by the [Integrated Climate Data Center (ICDC, CEN, University of Hamburg)](https://www.cen.uni-hamburg.de/en/icdc/data/ocean/ar5-slr.html). The data includes 10 geophysical sources that drive long-term changes in relative sea level change; 5 ice components, 3 ocean-related components, a land water storage and glacial isostatic adjustment. Also, we consider IPCC's latest medium confidence relative median regional sea level projections published in the Sixth Assessment Report (AR6, 2021) processed by [NASA's Jet Propulsion Laboratory](https://podaac.jpl.nasa.gov/announcements/2021-08-09-Sea-level-projections-from-the-IPCC-6th-Assessment-Report). The data includes antarctic ice sheet, greenland ice sheet, glaciers, land-water storage, ocean dynamics and vertical land motion as geophysical sources that drive long-term changes. \n",
    "\n",
    "The data is hosted in cloud buckets as Cloud Optimized GeoTIFFs (COGs). [COG](https://www.cogeo.org/) is a regular GeoTIFF file (viewed in for instance QGIS) but aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows in the cloud. You can basically ask for parts of the file you need which make post-processing routines very fast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:09:57.210534Z",
     "iopub.status.busy": "2023-08-09T13:09:57.210330Z",
     "iopub.status.idle": "2023-08-09T13:09:58.111968Z",
     "shell.execute_reply": "2023-08-09T13:09:58.111402Z",
     "shell.execute_reply.started": "2023-08-09T13:09:57.210513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# import holoviews as hv\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "\n",
    "# import xarray as xr\n",
    "import rioxarray as rio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoCliCo STAC catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:09:58.114212Z",
     "iopub.status.busy": "2023-08-09T13:09:58.114123Z",
     "iopub.status.idle": "2023-08-09T13:09:58.993577Z",
     "shell.execute_reply": "2023-08-09T13:09:58.993161Z",
     "shell.execute_reply.started": "2023-08-09T13:09:58.114203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the CoCliCo STAC catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://storage.googleapis.com/dgds-data-public/coclico/coclico-stac/catalog.json\"\n",
    ")\n",
    "# catalog\n",
    "\n",
    "# list the datasets present in the catalog, we are interested in the slp5 and slp6 sets\n",
    "list(catalog.get_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:09:59.892660Z",
     "iopub.status.busy": "2023-08-09T13:09:59.891895Z",
     "iopub.status.idle": "2023-08-09T13:09:59.897033Z",
     "shell.execute_reply": "2023-08-09T13:09:59.896424Z",
     "shell.execute_reply.started": "2023-08-09T13:09:59.892622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yr = 2100  # set year\n",
    "ens = 50  # set ensemble [0-100]\n",
    "var = \"slr\"  # set variable\n",
    "ccs5 = \"26\"  # set climate change scenario for AR5\n",
    "ccs6 = \"1-26\"  # set climate change scenario for AR5\n",
    "\n",
    "# CONTINUE\n",
    "# open href items with rio xarray to display them (first as normal plot then with holoviews?)\n",
    "# use parameter space to change the opened item for the plot\n",
    "# use the catalog content to make the plot nice (i.e. same bounding boxes, etc)\n",
    "# import land-water line (rough) to display land?\n",
    "\n",
    "# look at py-sense to have some examples\n",
    "# do we need a zarr file to easily query temporal information or can we do that by loading all cog information easily?\n",
    "# if so, then we would need an algorithm that resets the clicked point to the closest cell center to query from the zarr\n",
    "\n",
    "# can we do this all in an html or something that people can just slide without running the code??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:10:01.240135Z",
     "iopub.status.busy": "2023-08-09T13:10:01.239637Z",
     "iopub.status.idle": "2023-08-09T13:11:01.243425Z",
     "shell.execute_reply": "2023-08-09T13:11:01.242958Z",
     "shell.execute_reply.started": "2023-08-09T13:10:01.240101Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get data\n",
    "# TODO: how to speed this up for datapoints at the end to the ar5/6_col?\n",
    "\n",
    "# get AR5 collection and item href\n",
    "ar5_col = catalog.get_child(\"slp5\")\n",
    "ar5_item_href = (\n",
    "    ar5_col.get_item(\n",
    "        r\"rcp=%s/%s_ens%s/%s-01-01_%s-01-01.tif\" % (ccs5, var, int(ens), yr, yr + 1)\n",
    "    )\n",
    "    .assets[\"data\"]\n",
    "    .href\n",
    ")\n",
    "ar5_item = rio.open_rasterio(ar5_item_href, masked=True)\n",
    "\n",
    "# get AR6 collection and item href\n",
    "ar6_col = catalog.get_child(\"slp6\")\n",
    "ar6_item_href = (\n",
    "    ar6_col.get_item(r\"ssp=%s/%s_ens%s/%s.tif\" % (ccs6, var, float(ens), yr))\n",
    "    .assets[\"data\"]\n",
    "    .href\n",
    ")\n",
    "ar6_item = rio.open_rasterio(ar6_item_href, masked=True)\n",
    "ar6_item_corr = ar6_item / 1000\n",
    "\n",
    "# cbar limits\n",
    "vmin = max(\n",
    "    min(np.nanmin(ar5_item), np.nanmin(ar6_item_corr)), -0.2\n",
    ")  # bound to -0.2 if smaller than this value\n",
    "vmax = max(np.nanmax(ar5_item), np.nanmax(ar6_item_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:11:01.244637Z",
     "iopub.status.busy": "2023-08-09T13:11:01.244425Z",
     "iopub.status.idle": "2023-08-09T13:11:01.553896Z",
     "shell.execute_reply": "2023-08-09T13:11:01.553200Z",
     "shell.execute_reply.started": "2023-08-09T13:11:01.244623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# %matplotlib inline\n",
    "\n",
    "# TODO: zoom with ipyleaflet bbox converted to this plot?\n",
    "# TODO: zoom to same extent (sharex, sharey does not work properly) when selecting a boundings box in ipympl\n",
    "\n",
    "# define figure\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1, 2, figsize=(13, 4), subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    ")  # , sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "# plt.gcf().subplots_adjust(bottom=0.05)\n",
    "plt.gcf().subplots_adjust(left=0.05)\n",
    "\n",
    "# populate AR5 plot\n",
    "ax1.set_facecolor(\"pink\")\n",
    "im5 = ar5_item.plot(\n",
    "    ax=ax1,\n",
    "    add_colorbar=False,\n",
    "    vmin=round(vmin, 2),\n",
    "    vmax=round(vmax, 2),\n",
    "    cmap=plt.cm.afmhot_r,\n",
    ")\n",
    "ax1.set_title(\"%s \\nRCP%s, %s, %sth percentile\" % (ar5_col.title, ccs5, yr, ens))\n",
    "# ax1.set_xlabel(\"Longitude [Degrees East]\") # TODO: possibly import from file?\n",
    "# ax1.set_ylabel(\"Latitude [Degrees North]\") # TODO: possibly import from file?\n",
    "cbar5 = plt.colorbar(im5, shrink=0.675, aspect=30 * 0.675, pad=0.02)\n",
    "cbar5.set_label(\n",
    "    \"sea level rise [%s]\" % ar5_col.extra_fields[\"deltares:units\"]\n",
    ")  # TODO: possibly import from file?\n",
    "ax1.add_feature(cf.LAND, facecolor=\"lightgrey\", zorder=15)\n",
    "ax1.add_feature(cf.COASTLINE, linewidth=0.2, zorder=16)\n",
    "ax1.add_feature(cf.BORDERS, linewidth=0.1, zorder=16)\n",
    "gl1 = ax1.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True,\n",
    "    linewidth=1,\n",
    "    color=\"gray\",\n",
    "    alpha=0.2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "gl1.xlabels_top = False\n",
    "gl1.ylabels_right = False\n",
    "ax1.text(\n",
    "    -0.08,\n",
    "    0.5,\n",
    "    \"latitude\",\n",
    "    va=\"bottom\",\n",
    "    ha=\"center\",\n",
    "    rotation=\"vertical\",\n",
    "    rotation_mode=\"anchor\",\n",
    "    transform=ax1.transAxes,\n",
    ")\n",
    "ax1.text(\n",
    "    0.5,\n",
    "    -0.15,\n",
    "    \"longitude\",\n",
    "    va=\"bottom\",\n",
    "    ha=\"center\",\n",
    "    rotation=\"horizontal\",\n",
    "    rotation_mode=\"anchor\",\n",
    "    transform=ax1.transAxes,\n",
    ")\n",
    "\n",
    "# populate AR6 plot\n",
    "ax2.set_facecolor(\"pink\")\n",
    "im6 = ar6_item_corr.plot(\n",
    "    ax=ax2,\n",
    "    add_colorbar=False,\n",
    "    vmin=round(vmin, 2),\n",
    "    vmax=round(vmax, 2),\n",
    "    cmap=plt.cm.afmhot_r,\n",
    ")\n",
    "ax2.set_title(\"%s \\nSSP%s, %s, %sth percentile\" % (ar6_col.title, ccs6, yr, ens))\n",
    "# ax2.set_xlabel(\"Longitude [Degrees East]\") # TODO: possibly import from file?\n",
    "# ax2.set_ylabel(\"\") # leave empty\n",
    "cbar6 = plt.colorbar(im6, shrink=0.675, aspect=30 * 0.675, pad=0.02)\n",
    "cbar6.set_label(\n",
    "    \"sea level rise [%s]\" % ar6_col.extra_fields[\"deltares:units\"]\n",
    ")  # TODO: possibly import from file?\n",
    "ax2.add_feature(cf.LAND, facecolor=\"lightgrey\", zorder=15)\n",
    "ax2.add_feature(cf.COASTLINE, linewidth=0.2, zorder=16)\n",
    "ax2.add_feature(cf.BORDERS, linewidth=0.1, zorder=16)\n",
    "gl2 = ax2.gridlines(\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True,\n",
    "    linewidth=1,\n",
    "    color=\"gray\",\n",
    "    alpha=0.2,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "gl2.xlabels_top = False\n",
    "gl2.ylabels_right = False\n",
    "gl2.ylabels_left = False\n",
    "ax2.text(\n",
    "    0.5,\n",
    "    -0.15,\n",
    "    \"longitude\",\n",
    "    va=\"bottom\",\n",
    "    ha=\"center\",\n",
    "    rotation=\"horizontal\",\n",
    "    rotation_mode=\"anchor\",\n",
    "    transform=ax2.transAxes,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General feedback (Floris) \n",
    "\n",
    "1) Put attributes like ENS, YRS and CCS in the STAC. Floris most advanced example of adding COG's to a STAC collection can be found [here](https://github.com/FlorisCalkoen/coastmonitor/blob/main/notebooks/typology/02_dynamic_world_stac.ipynb) Have a look at the `create_item()` and `create_asset()` functions. \n",
    "2) By reading the STAC items with a library like odc-stac, stackstac or xpystac you shoul be able to have the band dimension as time with labeled coordinates, which should be faster for indexing. \n",
    "3) If that's still too slow, you can consider to rechunk the data spatially instead of temporallily. So the you will have spatial partitions, which each contain all the timestamps. But then it should be put in a zarr store I guess. So that would be the other option; put all data in a zarr store and consider whether its more useful to have spatial or temporal dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Xarray datasets\n",
    "\n",
    "Xarray datasets can be indexed geospatiaally using the rio accessor by geojson strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:18:56.237794Z",
     "iopub.status.busy": "2023-08-09T13:18:56.237014Z",
     "iopub.status.idle": "2023-08-09T13:18:56.250525Z",
     "shell.execute_reply": "2023-08-09T13:18:56.249878Z",
     "shell.execute_reply.started": "2023-08-09T13:18:56.237754Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely\n",
    "\n",
    "# select a point to plot timeseries of SLR projections for\n",
    "point_location = [4.2, 52.8]  # easting, norting\n",
    "\n",
    "# TODO: enable computing the mean of multiple cells in a polygon or specified set of points and show the SLR projection timeseries\n",
    "area_location = [4.4, 52.6, 5.4, 53]\n",
    "\n",
    "point_geom = shapely.Point(point_location)\n",
    "area_geom = shapely.box(*area_location)\n",
    "\n",
    "# geometry as json string that can be used to index xarray dataset using the\n",
    "# rio accessor\n",
    "shapely.geometry.mapping(area_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read STAC as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:20:46.099528Z",
     "iopub.status.busy": "2023-08-09T13:20:46.099285Z",
     "iopub.status.idle": "2023-08-09T13:20:48.841452Z",
     "shell.execute_reply": "2023-08-09T13:20:48.841153Z",
     "shell.execute_reply.started": "2023-08-09T13:20:46.099503Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from copy import deepcopy\n",
    "\n",
    "def items_to_dataframe(items: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"STAC items to Pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        items (List[Dict]): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    _items = []\n",
    "    for i in items:\n",
    "        _i = deepcopy(i)\n",
    "        # _i['geometry'] = shape(_i['geometry'])\n",
    "        # ...  # for example, drop some attributes that you're not interested in\n",
    "        _items.append(_i)\n",
    "    df = pd.DataFrame(pd.json_normalize(_items))\n",
    "    for field in [\"properties.datetime\"]:\n",
    "        if field in df:\n",
    "            df[field] = pd.to_datetime(df[field])\n",
    "    df = df.sort_values(\"properties.datetime\")\n",
    "    return df\n",
    "\n",
    "items = list(ar5_col.get_items())\n",
    "items_df = items_to_dataframe([i.to_dict() for i in items])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "Add more metadata to the STAC catalog that we can use to find the items that we are looking for. So instead of inferring the CCS, ENS and SLR from the links in the STAC they should be present as properties. Maybe under the Deltares/CoCliCo prefix or as additional properties. Then you can do something like this:\n",
    "\n",
    "```python\n",
    "indices = items_df.loc[(items_df[\"SLR\" == ...]) & (items_df[\"CCS\"] ==...)].index.to_list()\n",
    "selected_items = [items[i] for i in indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: doing the indexing yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:20:48.842320Z",
     "iopub.status.busy": "2023-08-09T13:20:48.842215Z",
     "iopub.status.idle": "2023-08-09T13:20:48.844667Z",
     "shell.execute_reply": "2023-08-09T13:20:48.844268Z",
     "shell.execute_reply.started": "2023-08-09T13:20:48.842313Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample indices from the DataFrame\n",
    "sample_indices = items_df.sample(n=2).index.tolist()\n",
    "\n",
    "# Get the items at those indices\n",
    "selected_items = [items[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:21:03.907436Z",
     "iopub.status.busy": "2023-08-09T13:21:03.907002Z",
     "iopub.status.idle": "2023-08-09T13:21:05.929567Z",
     "shell.execute_reply": "2023-08-09T13:21:05.929224Z",
     "shell.execute_reply.started": "2023-08-09T13:21:03.907411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "def preprocess(ds):\n",
    "    point_location = [4.2, 52.8]  # easting, norting\n",
    "    point_geom = shapely.Point(point_location)\n",
    "    point_mapping = shapely.geometry.mapping(point_geom)\n",
    "    \n",
    "    ds = (\n",
    "        ds.rio.clip([point_geom])\n",
    "        .sel(x=point_location[0], y=point_location[1], method=\"nearest\")\n",
    "        # .rename({\"band\": \"time\"})\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "selected_ds = xr.open_mfdataset(\n",
    "    [i.assets[\"data\"].href for i in selected_items],\n",
    "    engine=\"rasterio\",\n",
    "    concat_dim=\"band\",\n",
    "    combine=\"nested\",\n",
    "    preprocess=preprocess,  # this indexes every before merging them\n",
    "    parallel=True, # this is for Dask\n",
    ")\n",
    "# manually add the datetimes to dataset because they are not contained in the tiff\n",
    "times = [i.properties[\"datetime\"] for i in selected_items]\n",
    "selected_ds = selected_ds.rename({\"band\":\"time\"}).assign_coords(time=times)\n",
    "\n",
    "# the plot looks a bit weird because we took two random samples\n",
    "selected_ds[\"band_data\"].plot.line(x=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2:\n",
    "\n",
    "Use a library like odc-stac, stackstac or xpystac. The plot below ignores a lot of data because it probably combines the data based on the metadata in the STAC. In this metadata there are time duplicates because they contain different scenario's.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:21:12.188503Z",
     "iopub.status.busy": "2023-08-09T13:21:12.188109Z",
     "iopub.status.idle": "2023-08-09T13:21:33.614707Z",
     "shell.execute_reply": "2023-08-09T13:21:33.614429Z",
     "shell.execute_reply.started": "2023-08-09T13:21:12.188478Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from odc.stac import stac_load\n",
    "\n",
    "stac_load(items, chunks={}, lon=[4.19, 4.21], lat=[52.69, 52.71]).squeeze()[\n",
    "    \"data\"\n",
    "].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is previous nb from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T08:46:11.721640Z",
     "iopub.status.idle": "2023-08-09T08:46:11.721735Z",
     "shell.execute_reply": "2023-08-09T08:46:11.721683Z",
     "shell.execute_reply.started": "2023-08-09T08:46:11.721678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve AR5 and AR6 data at above-defined location\n",
    "# TODO: check if Zarr usage is easier here, takes quite some time to index a position from CoG files (https://github.com/intake/intake-stac/issues/66)..\n",
    "# TODO: what takes more time? get_items or reading the point in the tiff image? If the former, might work to use get_item_links and add string storage location manually\n",
    "\n",
    "# define variables\n",
    "ens_list = [\"5\", \"50\", \"95\"]  # ensemble list to look into\n",
    "yrs_list = np.arange(1970, 2200, 10)  # years to look into (step of 10 years from 1970)\n",
    "\n",
    "# loop over tifs to obtain data for AR5\n",
    "key_list = [\"CCS\", \"YRS\", \"ENS\", \"SLR\"]\n",
    "AR5_dict = {key: [] for key in key_list}\n",
    "for idx, (i, j) in tqdm(enumerate(zip(ar5_col.get_items(), ar5_col.get_item_links()))):\n",
    "    enss = str(j).split(\"/\")[1].split(\"ens\")[-1]  # ensemble\n",
    "    yrs = int(str(j).split(\"/\")[2][0:4])  # yrs\n",
    "    if enss in ens_list and yrs in yrs_list:  # constraining read ensembles and years\n",
    "        # print(i.assets[\"data\"].href)\n",
    "        AR5_dict[\"CCS\"].append(\n",
    "            str(i).split(\"/\")[0].split(\"=\")[-1]\n",
    "        )  # climate change scenario for AR5\n",
    "        AR5_dict[\"YRS\"].append(yrs)  # year, at start of year similar to AR6\n",
    "        AR5_dict[\"ENS\"].append(enss)  # append ensemble\n",
    "        # ar5_item = rio.open_rasterio(i.assets[\"data\"].href, masked=True)  # open item\n",
    "        # AR5_dict[\"SLR\"].append(\n",
    "        #     ar5_item.sel(x=loc[0], y=loc[1], method=\"nearest\").values[0]\n",
    "        # )  # match coordinate at center of raster cells\n",
    "\n",
    "# append to dataframe\n",
    "df5 = pd.DataFrame(data=AR5_dict)\n",
    "\n",
    "# loop over tifs to obtain data for AR6\n",
    "AR6_dict = {key: [] for key in key_list}\n",
    "for idx, (i, j) in tqdm(enumerate(zip(ar6_col.get_items(), ar6_col.get_item_links()))):\n",
    "    enss = str(j).split(\"/\")[1].split(\"ens\")[-1]  # ensemble\n",
    "    yrs = int(str(j).split(\"/\")[2][0:4])  # yrs\n",
    "    if (\n",
    "        enss in [str(float(x)) for x in ens_list] and yrs in yrs_list\n",
    "    ):  # constraining read ensembles and years\n",
    "        # print(i.assets[\"data\"].href)\n",
    "        AR6_dict[\"CCS\"].append(\n",
    "            str(i).split(\"/\")[0].split(\"=\")[-1]\n",
    "        )  # climate change scenario for AR5\n",
    "        AR6_dict[\"YRS\"].append(yrs)  # year, at start of year similar to AR6\n",
    "        AR6_dict[\"ENS\"].append(enss)  # append ensemble\n",
    "        ar6_item = rio.open_rasterio(i.assets[\"data\"].href, masked=True)  # open item\n",
    "        AR6_dict[\"SLR\"].append(\n",
    "            ar6_item.sel(x=loc[0], y=loc[1], method=\"nearest\").values[0] / 1000\n",
    "        )  # match coordinate at center of raster cells\n",
    "\n",
    "# append to dataframe\n",
    "df6 = pd.DataFrame(data=AR6_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-09T08:46:11.722313Z",
     "iopub.status.idle": "2023-08-09T08:46:11.722470Z",
     "shell.execute_reply": "2023-08-09T08:46:11.722419Z",
     "shell.execute_reply.started": "2023-08-09T08:46:11.722413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# %matplotlib inline\n",
    "\n",
    "# TODO: trial to make use of ChatGPT functionality to make plot with text explanation (https://github.com/gventuri/pandas-ai/blob/main/Notebooks/Getting%20Started.ipynb)\n",
    "\n",
    "# define figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4), sharey=True, sharex=True)\n",
    "fig.tight_layout()\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "plt.gcf().subplots_adjust(left=0.05)\n",
    "ax = plt.gca()\n",
    "\n",
    "# specify colors for plots, different for AR5 & AR6 following official IPCC figures\n",
    "colorsAR5 = [\"purple\", \"cyan\", \"red\"]\n",
    "colorsAR6 = [\"darkblue\", \"orange\", \"darkred\"]\n",
    "\n",
    "# AR5\n",
    "for idx, (scen, grp) in enumerate(df5.groupby([\"CCS\"])):  # group per scenario\n",
    "    ens_list = list(grp.groupby(\"ENS\"))\n",
    "    ens_list[1][1].plot(\n",
    "        kind=\"line\",\n",
    "        x=\"YRS\",\n",
    "        y=\"SLR\",\n",
    "        label=\"RCP%s\" % (scen[0][0] + \".\" + scen[0][1]),\n",
    "        color=colorsAR5[idx],\n",
    "        ax=ax1,\n",
    "        alpha=0.5,\n",
    "    )  # mean 50 percentile\n",
    "    ax1.fill_between(\n",
    "        ens_list[0][1].YRS,\n",
    "        ens_list[0][1].SLR,\n",
    "        ens_list[2][1].SLR,\n",
    "        alpha=0.1,\n",
    "        color=colorsAR5[idx],\n",
    "        interpolate=True,\n",
    "    )  # 5-95th percentile shading\n",
    "\n",
    "ax1.set_title(\n",
    "    \"AR5 sea level rise projections at location (lon, lat): %s, %s\" % (loc[0], loc[1])\n",
    ")\n",
    "ax1.xaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "ax1.yaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "ax1.axvline(2020, linestyle=\"--\", color=\"k\", linewidth=1, alpha=0.2)\n",
    "ax1.axvline(2100, linestyle=\"--\", color=\"k\", linewidth=1, alpha=0.2)\n",
    "ax1.set_xlim(2000, 2150)\n",
    "ax1.grid(alpha=0.2)\n",
    "ax1.set_xlabel(\"time [year]\")\n",
    "ax1.set_ylabel(\"sea level rise [m]\")\n",
    "\n",
    "# AR6\n",
    "for idx, (scen, grp) in enumerate(df6.groupby([\"CCS\"])):  # group per scenario\n",
    "    ens_list = list(grp.groupby(\"ENS\"))\n",
    "    ens_list[1][1].plot(\n",
    "        kind=\"line\",\n",
    "        x=\"YRS\",\n",
    "        y=\"SLR\",\n",
    "        label=\"SSP%s\" % (scen[0][0:3] + \".\" + scen[0][3]),\n",
    "        color=colorsAR6[idx],\n",
    "        ax=ax2,\n",
    "        alpha=0.5,\n",
    "    )  # mean 50 percentile\n",
    "    ax2.fill_between(\n",
    "        ens_list[0][1].YRS,\n",
    "        ens_list[0][1].SLR,\n",
    "        ens_list[2][1].SLR,\n",
    "        alpha=0.1,\n",
    "        color=colorsAR6[idx],\n",
    "        interpolate=True,\n",
    "    )  # 5-95th percentile shading\n",
    "\n",
    "ax2.set_title(\n",
    "    \"AR6 sea level rise projections at location (lon, lat): %s, %s\" % (loc[0], loc[1])\n",
    ")\n",
    "ax2.xaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "ax2.yaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "ax2.axvline(2020, linestyle=\"--\", color=\"k\", linewidth=1, alpha=0.2)\n",
    "ax2.axvline(2100, linestyle=\"--\", color=\"k\", linewidth=1, alpha=0.2)\n",
    "ax1.set_xlim(2000, 2150)\n",
    "ax2.grid(alpha=0.2)\n",
    "ax2.set_xlabel(\"time [year]\")\n",
    "ax2.set_ylabel(\"sea level rise [m]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comparison of AR5 / AR6 to the global mean at the selected location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: export to HTML to try out an interactive example in which you can alter the clicked point to look at SLR projections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:testenv]",
   "language": "python",
   "name": "conda-env-testenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
