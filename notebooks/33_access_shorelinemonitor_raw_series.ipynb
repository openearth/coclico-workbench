{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from coastmonitor.io.drive_config import configure_instance\n",
    "\n",
    "configure_instance(branch=\"dev\")\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"datatframe.query-planning\": False})\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import duckdb\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from coastmonitor.io.utils import read_items_extent\n",
    "from coastmonitor.query_engine import HREFQueryEngine, STACQueryEngine\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# NOTE: access tokens to the data are available upon request.\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_options = {\"account_name\": account_name, \"credential\": sas_token}\n",
    "\n",
    "# These are the URL's to the STAC catalog that we can use to efficiently index the data\n",
    "COCLICO_STAC_URL = \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    "\n",
    "# Global Coastal Transect System (publicly available and in review)\n",
    "GCTS_COLLECTION_NAME = \"gcts\"\n",
    "\n",
    "# Global Coastal Transect Repository (unreleased; access keys provided upon request). This dataset consists\n",
    "# of GCTS + several other characteristics, such as intersection distance to nearest coastline.\n",
    "GCTR_COLLECTION_NAME = \"gctr\"\n",
    "\n",
    "# ShorelineMonitor Raw Series (unreleased; access keys provided upon request). This dataset consists\n",
    "# ShorelineMonitor Shorlines that are mapped onto the Global Coastal Transect System (Raw Series) that\n",
    "# have a wide range of additional statistics used to filter out the primary, high-quality observations.\n",
    "SM_COLLECTION_NAME = \"shorelinemonitor-raw-series\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Read the STAC collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog = pystac.Catalog.from_file(COCLICO_STAC_URL)\n",
    "sm_collection = coclico_catalog.get_child(SM_COLLECTION_NAME)\n",
    "gcts_collection = coclico_catalog.get_child(GCTS_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Show the spatial extents of both collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_extents = read_items_extent(sm_collection)\n",
    "gcts_extents = read_items_extent(gcts_collection)\n",
    "sm_extents[[\"geometry\"]].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Create a interactive map that we use to define our region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 53.4, 5.4\n",
    "m.zoom = 11\n",
    "m.layout.height = \"800px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these coordiantes are extracted from the interactive map above\n",
    "minx, miny, maxx, maxy = m.west, m.south, m.east, m.north"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Create a DuckDB query engine to retrieve data from cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_engine = STACQueryEngine(\n",
    "    stac_collection=sm_collection,\n",
    "    storage_backend=\"azure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorelines = shoreline_engine.get_data_within_bbox(minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transects_engine = STACQueryEngine(\n",
    "    stac_collection=gcts_collection, storage_backend=\"azure\"\n",
    ")\n",
    "transects = transects_engine.get_data_within_bbox(minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = shorelines.loc[shorelines[\"shoreline_position\"].isna()].copy()\n",
    "s = shorelines.copy()\n",
    "last_obs = (\n",
    "    s.sort_values(by=[\"time\"])\n",
    "    .groupby(\"tr_name\")[\"shoreline_chainage\"]\n",
    "    .last()\n",
    "    .rename(\"last_obs\")\n",
    ")\n",
    "s = s.merge(last_obs, on=\"tr_name\", how=\"left\")\n",
    "s[\"shoreline_position\"] = s[\"shoreline_chainage\"] - s[\"last_obs\"]\n",
    "# s[\"shoreline_position\"] = s[\"shoreline_chainage\"] - s[\"last_obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~shorelines.obs_is_primary).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~shorelines.loc[shorelines[\"shoreline_position\"].isna()].obs_is_primary).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[\"last_obs\"].max("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[\"shoreline_position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def flag_obs_ht_max_step_change(\n",
    "    df: DataFrame,\n",
    "    max_step_change: float = 50,\n",
    "    max_n_step_changes: int = 4,\n",
    "    max_year_interval: int = 10,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detects significant step changes in shoreline positions across different transects within a DataFrame.\n",
    "    A step change is identified based on the difference between consecutive measurements exceeding a specified threshold.\n",
    "\n",
    "    The function also flags entire transects as 'unsteady' if the number of step changes exceeds a certain threshold.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame containing shoreline position data. The DataFrame must include\n",
    "                        the columns 'time', 'tr_name', 'shoreline_position', and 'geometry'.\n",
    "        max_step_change (float): Threshold for detecting significant step changes. Default is 50.\n",
    "        max_n_step_changes (int): Maximum number of step changes allowed per transect before flagging as 'unsteady'. Default is 4.\n",
    "        max_year_interval (int): Maximum year difference for considering a step change significant. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A modified copy of the input DataFrame with additional columns indicating detected\n",
    "                   step changes ('obs_ht_max_step_change') and unsteady transects ('tr_is_unsteady').\n",
    "    \"\"\"\n",
    "    # Copy relevant columns\n",
    "    df = df[[\"time\", \"tr_name\", \"shoreline_position\", \"geometry\"]].copy()\n",
    "\n",
    "    # Calculate differences and year shifts\n",
    "    df[\"backward_diff\"] = df.groupby(\"tr_name\")[\"shoreline_position\"].diff()\n",
    "    df[\"forward_diff\"] = df[\"backward_diff\"].shift(-1)\n",
    "    df[\"year\"] = df[\"time\"].dt.year\n",
    "    df[\"dt_backward\"] = df.groupby(\"tr_name\")[\"year\"].diff()\n",
    "    df[\"dt_forward\"] = df[\"dt_backward\"].shift(-1)\n",
    "\n",
    "    # Detect step changes in the middle of the time series\n",
    "    df[\"mid_step_change\"] = (\n",
    "        (df[\"backward_diff\"].abs() > max_step_change)\n",
    "        & (df[\"forward_diff\"].abs() > max_step_change)\n",
    "        & (df[\"backward_diff\"] * df[\"forward_diff\"] < 0)\n",
    "        & (df[\"dt_backward\"] < max_year_interval)\n",
    "        & (df[\"dt_forward\"] < max_year_interval)\n",
    "    )\n",
    "\n",
    "    # Group by the observations per transect\n",
    "    g = df.groupby(\"tr_name\")[[\"tr_name\", \"backward_diff\", \"forward_diff\"]]\n",
    "\n",
    "    # Get the first observation\n",
    "    first = g.nth(0).reset_index(drop=False).set_index(\"tr_name\")\n",
    "    # First observation is a step change if the first difference exceeds max_step_change and is followed by a small difference\n",
    "    first[\"first_step_change\"] = (first[\"forward_diff\"].abs() > max_step_change) & (\n",
    "        g.nth(1).set_index(\"tr_name\")[\"forward_diff\"] < max_step_change\n",
    "    )\n",
    "    # Merge the flag to the primary DataFrame\n",
    "    df[\"first_step_change\"] = first.set_index(\"index\")[\"first_step_change\"]\n",
    "\n",
    "    # Get the last observation\n",
    "    last = g.tail(1).reset_index(drop=False).set_index(\"tr_name\")\n",
    "    return last\n",
    "    # Last observation is a step change if the last difference exceeds max_step_change and is preceded by a small difference\n",
    "    last[\"last_step_change\"] = (last[\"backward_diff\"].abs() > max_step_change) & (\n",
    "        g.tail(2).iloc[::2].set_index(\"tr_name\")[\"backward_diff\"].abs()\n",
    "        < max_step_change\n",
    "    )\n",
    "    # Merge the flag to the primary DataFrame\n",
    "    df[\"last_step_change\"] = last.set_index(\"index\")[\"last_step_change\"]\n",
    "\n",
    "    # Combine mid, first, and last step change flags\n",
    "    df[\"obs_ht_max_step_change\"] = (\n",
    "        df[\"mid_step_change\"] | df[\"first_step_change\"] | df[\"last_step_change\"]\n",
    "    )\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"backward_diff\",\n",
    "            \"forward_diff\",\n",
    "            \"mid_step_change\",\n",
    "            \"first_step_change\",\n",
    "            \"last_step_change\",\n",
    "            \"dt_backward\",\n",
    "            \"dt_forward\",\n",
    "            \"year\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Count the number of step changes per transect\n",
    "    tr_step_change = df.groupby(\"tr_name\")[\"obs_ht_max_step_change\"].sum().reset_index()\n",
    "    \"obs_ht_max_step_change\"\n",
    "\n",
    "    # Flag transects that have step changes exceeding the allowed number of step changes\n",
    "    tr_step_change[\"tr_is_unsteady\"] = (\n",
    "        tr_step_change[\"obs_ht_max_step_change\"] >= max_n_step_changes\n",
    "    )\n",
    "\n",
    "    # Merge \"tr_is_unsteady\" to the DataFrame\n",
    "    df = df.merge(tr_step_change[[\"tr_name\", \"tr_is_unsteady\"]], on=\"tr_name\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_shorelinemonitor_series(\n",
    "    df: DataFrame,\n",
    "    columns: List[str] = [\"time\", \"tr_name\", \"shoreline_position\", \"geometry\"],\n",
    "    sinuosity_threshold: float = 10,\n",
    "    mdn_offset_multiplier: float = 3,\n",
    "    min_obs_count: int = 5,\n",
    "    max_step_change: float = 50,\n",
    "    max_n_step_changes: int = 4,\n",
    "    max_year_interval: int = 10,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters shoreline position data based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Raw shoreline position data with required columns:\n",
    "                        'shoreline_sinuosity', 'is_shoal', 'obs_is_primary',\n",
    "                        'tr_is_qa', 'mdn_offset', 'tr_stdev', 'obs_is_outlier',\n",
    "                        'obs_count', 'time', 'tr_name', 'shoreline_position', 'geometry'.\n",
    "        columns (List[str]): List of columns to include in the cleaned DataFrame. Default is [\"time\", \"tr_name\", \"shoreline_position\", \"geometry\"].\n",
    "        sinuosity_threshold (float): Threshold for sinuosity. Default is 10.\n",
    "        mdn_offset_multiplier (float): Multiplier for the standard deviation to filter based on median offset. Default is 3.\n",
    "        min_obs_count (int): Minimum observation count per transect. Default is 5.\n",
    "        max_step_change (float): Threshold for detecting significant step changes. Default is 50.\n",
    "        max_n_step_changes (int): Maximum number of step changes allowed per transect before flagging as 'unsteady'. Default is 4.\n",
    "        max_year_interval (int): Maximum year difference for considering a step change significant. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned shoreline positions with selected columns and recalculated observation count per transect.\n",
    "    \"\"\"\n",
    "    # Filtering criteria for clean shoreline positions\n",
    "    df = df[\n",
    "        (df[\"shoreline_sinuosity\"] < sinuosity_threshold)\n",
    "        & (~df[\"is_shoal\"])\n",
    "        & (df[\"obs_is_primary\"])\n",
    "        & (df[\"tr_is_qa\"])\n",
    "        & (df[\"mdn_offset\"] < mdn_offset_multiplier * df[\"tr_stdev\"])\n",
    "        & (df[\"obs_count\"] >= min_obs_count)\n",
    "        & (df[\"obs_is_outlier\"] != 1)\n",
    "    ].copy()\n",
    "\n",
    "    # Detect and flag step changes\n",
    "    df = flag_obs_ht_max_step_change(\n",
    "        df,\n",
    "        max_step_change=max_step_change,\n",
    "        max_n_step_changes=max_n_step_changes,\n",
    "        max_year_interval=max_year_interval,\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "    # # DEBUG\n",
    "    # df = df.loc[(~df[\"tr_is_unsteady\"]) & (~df[\"obs_ht_max_step_change\"])]\n",
    "\n",
    "    # # Count the clean observations on each transect\n",
    "    # obs_count = (\n",
    "    #     df.groupby(\"tr_name\")[\"shoreline_position\"]\n",
    "    #     .count()\n",
    "    #     .rename(\"obs_count\")\n",
    "    #     .reset_index()\n",
    "    # )\n",
    "    # df = df.merge(obs_count, on=\"tr_name\")\n",
    "\n",
    "    # # # Organize the clean shoreline position DataFrame\n",
    "    # # df = (\n",
    "    # #     df.rename(columns={\"shoreline_position\": \"shoreline_position\"})\n",
    "    # #     .reset_index(drop=True)\n",
    "    # # )\n",
    "\n",
    "    # return df[columns]\n",
    "\n",
    "\n",
    "df_clean = clean_raw_shorelinemonitor_series(\n",
    "    shorelines,\n",
    "    columns=[\"time\", \"tr_name\", \"shoreline_position\", \"geometry\", \"obs_count\"],\n",
    "    sinuosity_threshold=10,\n",
    "    mdn_offset_multiplier=3,\n",
    "    min_obs_count=5,\n",
    "    max_step_change=50,\n",
    "    max_n_step_changes=4,\n",
    "    max_year_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg, signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ols_AC(group):\n",
    "    \"\"\"\n",
    "    Performs ordinary least squares (OLS) regression to find the slope and intercept\n",
    "    of shoreline positions as a function of time for a given group of data.\n",
    "\n",
    "    Parameters:\n",
    "        group (DataFrame): A pandas DataFrame containing the columns 'shoreline_position' and 'time',\n",
    "                           where 'time' is a datetime object.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - A tuple containing the intercept and slope of the regression line (p),\n",
    "            - The sum of the squared residuals of the regression (res).\n",
    "\n",
    "    Notes:\n",
    "        The design matrix is constructed with a constant term and a linear term for the year extracted\n",
    "        from the 'time' datetime object.\n",
    "    \"\"\"\n",
    "    y = group.shoreline_position.values\n",
    "    x = group.time.dt.year.values  # - 1984)\n",
    "\n",
    "    # create design matrix\n",
    "    M = x[:, np.newaxis] ** [0, 1]\n",
    "\n",
    "    # calculate least square solution\n",
    "    p, res, _, _ = linalg.lstsq(M, y)\n",
    "\n",
    "    return p, res\n",
    "\n",
    "\n",
    "def all_AC(shoreline_positions):\n",
    "    \"\"\"\n",
    "    Applies OLS regression across groups of shoreline position data, each group identified by 'tr_name',\n",
    "    and aggregates the results into a DataFrame.\n",
    "\n",
    "    This function iterates over each transect name group, performs OLS regression if the group has more than\n",
    "    two data points, and collects the regression coefficients and residuals.\n",
    "\n",
    "    Parameters:\n",
    "        shoreline_positions (DataFrame): A pandas DataFrame containing 'shoreline_position', 'time', and 'tr_name',\n",
    "                                         where 'time' must be a datetime object and 'tr_name' is the identifier\n",
    "                                         for each group.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with columns 'tr_name' for the transect names, 'intercept' and 'rate' for the\n",
    "                   OLS regression coefficients, and 'residues' for the sum of squared residuals of each regression.\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    intercepts = []\n",
    "    slopes = []\n",
    "    residues = []\n",
    "\n",
    "    for name, group in tqdm(\n",
    "        shoreline_positions.groupby(\"tr_name\"),\n",
    "        total=shoreline_positions.tr_name.unique().size,\n",
    "    ):\n",
    "        # for name, group in shoreline_positions.groupby(\"tr_name\"):\n",
    "        if group.obs_count.iloc[0] > 5:\n",
    "            p, res = ols_AC(group)\n",
    "\n",
    "            names.append(name)\n",
    "            intercepts.append(p[0])\n",
    "            slopes.append(p[1])\n",
    "            residues.append(res)\n",
    "\n",
    "    ols_ = pd.DataFrame(\n",
    "        {\n",
    "            \"tr_name\": names,\n",
    "            \"intercept\": intercepts,\n",
    "            \"rate\": slopes,\n",
    "            \"residues\": residues,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ols_\n",
    "\n",
    "\n",
    "ambient_change = all_AC(df_clean)\n",
    "ambient_change = ambient_change.merge(\n",
    "    transects[[\"tr_name\", \"geometry\"]],\n",
    "    on=\"tr_name\",\n",
    "    how=\"left\",\n",
    ")\n",
    "ambient_change = gpd.GeoDataFrame(ambient_change, crs=4326)[\n",
    "    [\"rate\", \"geometry\", \"tr_name\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ambient_change.hvplot(\n",
    "    kind=\"line\",\n",
    "    # x=\"alongshore_dist_km\",\n",
    "    y=[\"rate\"],\n",
    "    # groupby=[\"coastline\"],\n",
    "    xlabel=\"Alongshore Distance [km]\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "ambient_change.hvplot(\n",
    "    geo=True,\n",
    "    tiles=\"ESRI\",\n",
    "    color=\"rate\",\n",
    "    line_width=3,\n",
    "    title=\"Transects Colored by Rate\",\n",
    "    width=800,\n",
    "    colorbar=True,\n",
    "    cnorm=\"linear\",\n",
    "    cmap=cc.CET_D3[::-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoSeries.from_xy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal] *",
   "language": "python",
   "name": "conda-env-coastal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
