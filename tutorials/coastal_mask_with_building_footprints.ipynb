{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mapping Coastal Building Footprints\n",
    "\n",
    "This notebook was written for the CoCliCo meet-up in OrlÃ©ans,  October 2022, to demonstrate the CoCliCo STAC API Demonstration. \n",
    "\n",
    "The purpose of this notebook is to provide an example of how data that is available in the CoCliCo STAC catalog can be combined with data from other parties to do cloud-based data analysis.\n",
    "\n",
    "In this dataset we combine the coastal mask (Daniel Lincke, WCF, 2022) with buildings footprints [dataset](https://github.com/microsoft/GlobalMLBuildingFootprints) from Microsoft for The Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# make coclico library importable by appending src from project root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "PROJ_ROOT = os.path.dirname(cwd)\n",
    "\n",
    "sys.path.append(os.path.join(PROJ_ROOT, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Python tools\n",
    "\n",
    "Below we import some python packages that are often used when working with geospatial data that are hosted in the cloud. These libraries are listed a `environment.yml` file that is available in the root of coclico workbench repository. When the conda package manager is available on the machine (or better: mamba), a new environment with these packages can be created by running `conda create -n coclico -f environment.yml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"{PROJ_ROOT}/environment.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_geopandas\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Connect to the coclico-stac catalog\n",
    "\n",
    "The STAC catalog is currenlty hosted in our data bucket at https://storage.googleapis.com/dgds-data-public/coclico/coclico-stac/catalog.json. In future we could (and maybe should) move this to  https://coclicoservices.eu/api/stac/v1\n",
    "\n",
    "With `pystac` the json files that describe the coclico data following STAC specifications can be read into a Python object that is browsable and searchable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://storage.googleapis.com/dgds-data-public/coclico/coclico-stac/catalog.json\"\n",
    ")\n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Structure of STAC catalogues \n",
    "\n",
    "- Catalog: A STAC catalog is a top-level object that logically groups other Catalog, Collection, and item objects. \n",
    "- Collection: A STAC Collection provides additional information about a spatio-temporal collection of data. \n",
    "- Item: a STAC Item represents an atomic collection of inseparable data and metadata.\n",
    "- Asset: Links to data. \n",
    "\n",
    "For more please refer to the [documentation](https://stacspec.org/en/about/stac-spec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Browse the catalog\n",
    "\n",
    "There are several methods to browse the STAC catalog. Noteworthy is the search method `catalog.search()`, which is able to search for bounding boxes or tags. When datasets are created by the provides they can provide a list of \"tags\" which can be added to the stac. In that way users can for example browse all datasets related to \"sea-level-rise\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just list all collections that are present in the catalog\n",
    "list(catalog.get_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Get the items of a given dataset\n",
    "Collections may include several items, for example, each representing one Cloud Optimized GeoTiff. Here we make a list of STAC items that are included in the Coastal Mask collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now browse the datasets by interacting with the STAC catalog\n",
    "items = list(catalog.get_child(\"cm\").get_all_items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### What are STAC items?\n",
    "\n",
    "Nothing more than text files describing what's in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"{sys.getsizeof(items[0].to_dict())} bytes\")\n",
    "items[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0].to_dict()[\"assets\"][\"cm\"][\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Particular advantage of working with STAC: indexing datasets without having to read the data\n",
    "\n",
    "\n",
    "The aim here is to retrieve a list of items that contain data of our region of interest. So first we have to define a region of interest \n",
    "\n",
    "In the cell below, we import a function from the coclico-workbench tools for creating a bounding box geometry, wrapped inside a GeoPandas GeoDataFrame. This geometry is created from a list of bbox coordinates (yuor ROI). \n",
    "\n",
    "Working with GeoPandas dataframes is often handy because the library provides spatial indices to the dataframe, thus, data can be accessed using rtree functionalities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coclico.utils.geo import geo_bbox\n",
    "\n",
    "roi = geo_bbox(\n",
    "    [5.2774, 53.0838, 7.1232, 53.5638]\n",
    ")  # rois can be drawn using for example https://boundingbox.klokantech.com/\n",
    "roi.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Get the bounding box for each of the STAC items \n",
    "\n",
    "All the STAC items contain a property that describes the bounding bbox of the data. All these properties are converted to GeoPandas dataframes, which are concatenated into one dataframe.\n",
    "\n",
    "This dataframe describes the spatial extent per bounding box. When the source data (`coastal_mask.tif`) was converted to Cloud Optimized GeoTiff's  we chose a size of 512 x 512 pixels. The coastal mask was derived from the SRTM Dem, so each pixel equivalents approximately 90 meters. Thus, one of these bounding boxes span an area of 45 x 45 km, although the outer boxes may contain less pixels.  \n",
    "\n",
    "Note, here we construct the spatial extent from the STAC items by reading the bbox property in each item. In future, when search functions are implemenented, this will be even easier, as we can use the `pystac_client.Client().search()` method that can handle geometries such as bounding boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = pd.concat([geo_bbox(i.to_dict()[\"bbox\"]) for i in items])\n",
    "bboxes = bboxes.reset_index(drop=True)\n",
    "bboxes.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Get items for region of interest\n",
    "\n",
    "Suppose that only that from a certain region is required. In such scenario a bounding box of the region of interest (ROI) can be created using several gis tools, [also online](https://boundingbox.klokantech.com). With GeoPandas it is now easy to get the bboxes that match the ROI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_roi = gpd.sjoin(bboxes, roi)[bboxes.columns]\n",
    "bboxes_roi.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain STAC items that cover the ROI\n",
    "items_roi = [items[i] for i in bboxes_roi.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first item\n",
    "items_roi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Lazy loading with Dask \n",
    "\n",
    "Dask is useful to computations with data that exceed available memory on a machine because it is using lazy execution and iterating over the workload per partition. \n",
    "\n",
    "Insetad of listing all items we now list all hrefs available in the Coastal mask. These hrefs point to the blob objects in the data bucket (in this case COG's). Many libraries are able to read data directly from these hrefs (e.g., `Pandas, GeoPandas, Xarray, Dask and Dask_GeoPandas`. The list of href's is read with dase and opened using the rasterio backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_items = list(catalog.get_child(\"cm\").get_all_items())\n",
    "cm_hrefs = [i.assets[\"cm\"].href for i in cm_items]\n",
    "cm_hrefs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Launch a Dask cluster\n",
    "\n",
    "Bit beyond the scope of this talk.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(threads_per_worker=1, local_directory=\"/tmp\")\n",
    "print(client)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@dask.delayed\n",
    "def lazy_open(href):\n",
    "    chunks = dict(band=1, x=512, y=512)\n",
    "    return xr.open_dataset(href, chunks=chunks, engine=\"rasterio\")\n",
    "\n",
    "\n",
    "das = dask.compute(*[lazy_open(href) for href in cm_hrefs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = xr.combine_by_coords(das).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import hvplot.xarray  # qa\n",
    "\n",
    "dataset.squeeze(\"band\").hvplot.image(rasterize=True, x=\"x\", y=\"y\", aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Combine the coastal mask with building footprints\n",
    "\n",
    "Microsoft planetary computer hosts a building footprint dataset obtained from very-high resolution satellite imagery using NN's. Here we combine these building footprints with the coastal mask to calculate the building area per flood risk unit. \n",
    "\n",
    "Interacting with Planetary computer is beyond the scope of this tutorial, so a sample of the buildings is stored in the coclico data bucket for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas\n",
    "\n",
    "buildings_ddf = dask_geopandas.read_parquet(\n",
    "    \"gs://dgds-data-public/coclico/building_footprints_netherlands/\"\n",
    ")\n",
    "\n",
    "buildings_ddf[\"building_area\"] = buildings_ddf.to_crs(3857).geometry.area / 1e6\n",
    "buildings_ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Building footprints from MS Planetary computer\n",
    "\n",
    "For some regions these data are very accurate. Especially in remote areas they can be useful. On the other hand.. other regions might be missing without clear cause. It woud be useful to integrate this dataset with OSM building footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_ddf.partitions[0].compute().iloc[:3000].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Vectorize coastal mask data\n",
    "\n",
    "The coastal mask data are rasters, but to overlay this with the building footprints dataset, either the rasters data has to be vectorized or the building data has to be rasterized. Here we will vectorize the coastal mask dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from geopandas.array import GeometryDtype\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def lazy_vectorize(href):\n",
    "    with rasterio.open(href) as src:\n",
    "        # TODO: change default nodata values in source tif because mask=True does not work now\n",
    "        # currently nodata values are float(\"nan\")\n",
    "        data = src.read(1)  # could be src.read(1, mask=True)\n",
    "\n",
    "        # returns boolean array with true's for nan values\n",
    "        mask = np.isnan(data)\n",
    "\n",
    "        # Create vectors around the raster values that are not float(\"nan\") (masked).\n",
    "        # The rasterio.features.shapes function from rasterio returns dictionaries in geojson\n",
    "        # format of the geometry with the associated value (will always be 1 in this\n",
    "        # dataset. Shapely.geometry.shape can be used to convert the geojson dictionary\n",
    "        # to a geometry. The transform (affine) is used to map the raster values to the\n",
    "        # cartesian coordinates.\n",
    "        shape_generator = [\n",
    "            (shapely.geometry.shape(s), v)\n",
    "            for s, v in rasterio.features.shapes(\n",
    "                data, mask=~mask, transform=src.transform\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # two different methods (doesn't really make a difference):\n",
    "        # 1) generator to dictionary..\n",
    "        # store the geometries and associated values in a dictionary\n",
    "        data = dict(zip([\"geometry\", \"value\"], zip(*shape_generator)))\n",
    "\n",
    "        # cannot construct geodataframe from empty dictionary, so return empty (works with dask)\n",
    "        if not data:\n",
    "            return\n",
    "\n",
    "        gdf = gpd.GeoDataFrame.from_dict(data, crs=src.crs)\n",
    "\n",
    "    #         # 2) or unpack generator in two lists and construct dataframe from that at\n",
    "    #         vectors = list(shape_generator)\n",
    "    #         # Extract the polygon coordinates and values from the list\n",
    "    #         polygons = [polygon for polygon, _ in vectors]\n",
    "    #         values = [value for _, value in vectors]\n",
    "\n",
    "    #         # Create a geopandas dataframe populated with the polygon shapes\n",
    "    #         gdf = gpd.GeoDataFrame(data={\"values\": values}, geometry=polygons, crs=src.crs)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "meta = {\"geometry\": GeometryDtype, \"value\": float}\n",
    "coastal_mask_vec = dask.compute(*[lazy_vectorize(href) for href in cm_hrefs], meta=meta)\n",
    "coastal_mask = pd.concat(coastal_mask_vec)\n",
    "coastal_mask.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Preprocess coastal mask\n",
    "\n",
    "Drop small regions and add a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "coastal_mask = pd.concat(coastal_mask_vec)\n",
    "coastal_mask = coastal_mask.to_crs(3857)  # areas should be computed in p\n",
    "coastal_mask[\"cm_area\"] = coastal_mask.area / 1e6  # compute area in sq km\n",
    "coastal_mask = coastal_mask.loc[\n",
    "    coastal_mask[\"cm_area\"] > 1\n",
    "].copy()  ## only keep flood risk units greater than 1 sq km\n",
    "coastal_mask = coastal_mask.reset_index(drop=True)\n",
    "coastal_mask[\"cm_name\"] = [\n",
    "    \"\".join(str(uuid.uuid4()).split(\"-\")) for i in range(len(coastal_mask.index))\n",
    "]\n",
    "coastal_mask = coastal_mask.to_crs(4326)\n",
    "coastal_mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Spatial join using Dask Geopandas\n",
    "\n",
    "Load the vectorized coastal maks into Dask GeoPandas so that its spatial join methods can be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_mask_ddf = dask_geopandas.from_geopandas(coastal_mask, npartitions=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Compute building footprints per coastal mask region\n",
    "\n",
    "The computation is only triggered when we call `compute()`. The dashboard link provides a GUI monitor the task, including things such as task bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(client.dashboard_link)\n",
    "joined_ddf = buildings_ddf.sjoin(coastal_mask_ddf)\n",
    "joined = joined_ddf.compute()\n",
    "joined = joined.loc[joined.geometry.is_valid].drop(\"index_right\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Visualize results for ROI\n",
    "\n",
    "The data will be plotted using a datashader as the ROI still contains 350K buildings. The geometries have to be converted to a Spatial pandas GeoDataFrame before datashader is able to handle them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_roi = gpd.sjoin(joined, roi)[joined.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Make a visualization of the results\n",
    "\n",
    "Here we only show the overlay for the region of interest, as GeoViews requires SpatialPandas GeoDatFrames. Converting the geometries to from GeoPandas GeoDataFrame's to SpatialPandas geometries takes some time without dask. If you want the image for the whole Netherlands, change to `sp_joined_roi = sp.GeoDataFrame(joined)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatialpandas as sp\n",
    "\n",
    "sp_coastal_mask_roi = sp.GeoDataFrame(\n",
    "    gpd.sjoin(coastal_mask, roi)[coastal_mask.columns]\n",
    ")\n",
    "sp_joined_roi = sp.GeoDataFrame(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "import geoviews as gv\n",
    "import holoviews.operation.datashader as hd\n",
    "\n",
    "gv.extension(\"bokeh\")\n",
    "base_layer_hv = gv.tile_sources.EsriTerrain.opts(\n",
    "    width=1200, height=750, bgcolor=\"black\"\n",
    ")\n",
    "coastal_mask_hv = gv.Polygons(sp_coastal_mask_roi.geometry).opts(\n",
    "    fill_color=\"gray\", alpha=0.3, line_color=None\n",
    ")\n",
    "building_footprints_hv = hd.rasterize(gv.Polygons(sp_joined_roi.geometry)).opts(\n",
    "    alpha=1, colorbar=True, clabel=\"Building footprint density\", cmap=cc.fire\n",
    ")\n",
    "base_layer_hv * coastal_mask_hv * building_footprints_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0ebfe0da1198dfcf84fb64f221c7b7e35641f7728e9dc12fbc80e97aca2df12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
